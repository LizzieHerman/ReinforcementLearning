L-track.txt
Recommended acceleration: (0,-1)
Next cell should be (1,1)
Reward for next state: 0
Choosing action at next state: 3
Updated Q-value for action at initial state: 0.0
Choosing action: 3
Recommended acceleration: (-1,0)
Next cell should be (0,2)
Reward for next state: -1000
Choosing action at next state: 0
Updated Q-value for action at initial state: -1000.0
Recommended acceleration: (0,1)
Next cell should be (1,3)
Reward for next state: -1
Choosing action at next state: 0
Updated Q-value for action at initial state: 0.0
Choosing action: 1
Recommended acceleration: (1,0)
Next cell should be (2,2)
Reward for next state: -1
Choosing action at next state: 0
Updated Q-value for action at initial state: 0.0
Recommended acceleration: (0,1)
Next cell should be (2,3)
Reward for next state: -1
Choosing action at next state: 3
Updated Q-value for action at initial state: 0.0
Choosing action: 0
Recommended acceleration: (0,1)
Next cell should be (3,3)
Reward for next state: -1
Choosing action at next state: 3
Updated Q-value for action at initial state: 0.0
Choosing action: 3
Recommended acceleration: (-1,0)
Next cell should be (3,3)
Reward for next state: -1
Choosing action at next state: 1
Updated Q-value for action at initial state: 0.0
Choosing action: 3
Recommended acceleration: (-1,0)
Next cell should be (3,4)
Reward for next state: -1
Choosing action at next state: 0
Updated Q-value for action at initial state: 0.0
Recommended acceleration: (1,0)
Next cell should be (2,2)
Reward for next state: -1
Choosing action at next state: 0
Updated Q-value for action at initial state: 0.0
Choosing action: 3
Recommended acceleration: (-1,0)
Next cell should be (0,2)
Reward for next state: -1000
Choosing action at next state: 3
Updated Q-value for action at initial state: -1000.0
Recommended acceleration: (-1,0)
Next cell should be (0,2)
Reward for next state: -1000
Choosing action at next state: 1
Updated Q-value for action at initial state: -1000.0
Recommended acceleration: (1,0)
Next cell should be (2,2)
Reward for next state: -1
Choosing action at next state: 0
Updated Q-value for action at initial state: 0.0
Recommended acceleration: (0,1)
Next cell should be (2,3)
Reward for next state: -1
Choosing action at next state: 0
Updated Q-value for action at initial state: 0.0
Choosing action: 3
Recommended acceleration: (-1,0)
Next cell should be (2,3)
Reward for next state: -1
Choosing action at next state: 3
Updated Q-value for action at initial state: 0.0
Choosing action: 0
Recommended acceleration: (0,1)
Next cell should be (3,5)
Reward for next state: -1
Choosing action at next state: 3
Updated Q-value for action at initial state: 0.0
Recommended acceleration: (0,-1)
Next cell should be (1,1)
Reward for next state: 0
Choosing action at next state: 2
Updated Q-value for action at initial state: 0.0
Choosing action: 2
Recommended acceleration: (0,-1)
Next cell should be (1,1)
Reward for next state: 0
Choosing action at next state: 2
Updated Q-value for action at initial state: 0.0
Recommended acceleration: (0,-1)
Next cell should be (1,0)
Reward for next state: -1000
Choosing action at next state: 1
Updated Q-value for action at initial state: 0.0
Recommended acceleration: (0,-1)
Next cell should be (1,1)
Reward for next state: 0
Choosing action at next state: 0
Updated Q-value for action at initial state: 0.0
Recommended acceleration: (0,1)
Next cell should be (1,2)
Reward for next state: -1
Choosing action at next state: 1
Updated Q-value for action at initial state: 0.0
Choosing action: 0
Recommended acceleration: (0,1)
Next cell should be (1,2)
Reward for next state: -1
Choosing action at next state: 2
Updated Q-value for action at initial state: 0.0